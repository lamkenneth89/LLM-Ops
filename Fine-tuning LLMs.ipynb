{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fine tuning LLMs at scale. Utilize GPU to speed-up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset \"gpu\" in spark.conf.get(\"spark.databricks.clusterUsageTags.sparkVersion\"), \"THIS NOTEBOOK REQUIRES THAT A GPU RUNTIME IS UTILIZED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSTALL DeepSpeed package. install cuda directoryb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install deepspeed=0.9.1 py-cpuinfo==.0.0\n",
    "#%pip install tempfile\n",
    "#$pip install datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create local temmp director on the driver, as the root dir. for the model chckpoints during training\n",
    "import tempfile\n",
    "tmpdir = tempfile.TemporaryDirectory()\n",
    "local_training_root = tmpdir.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import transformers as tr\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data prep\n",
    "\n",
    "imdb_ds = load_dataset(\"imdb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the pre-trained model: t5 in this example\n",
    "model_checkpoint = \"t5-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the tokenizer that was used for the t5-small model\n",
    "tokenizer = tr.AutoTokenizer.from_pretrained(model_checkpoint, cache_dir=DA.paths.datasets) #use a pre-cached model\n",
    "\n",
    "## not applicable to non Databricks environment...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the binary sentiment dataset of IMDB to strings of negative, unknown and positive so that text-to-text model T5 can use\n",
    "\n",
    "def to_tokens(tokenizer: tr.models.t5.tokenization_t5_fast.T5TokenizerFast, label_map:dict) -> callable:\n",
    "  \"\"\"\"\n",
    "  Give a 'tokenizer' this closure will iterate through 'x' and return the result of 'apply()',\n",
    "  This function is mapped to a dataset and returned with ids and attention masks.\n",
    "  \"\"\"\n",
    "\n",
    "    def apply(x) -> tr.tokenization_utils_base.BatchEncoding:\n",
    "        \"\"\" From a formatted dataset 'x' a batch encoding 'token_res' is created.\"\"\"\n",
    "        target_label = [label_map[y] for y in x['label']]\n",
    "        token_res=tokenizer(x[text], text_target=target_labels ,return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        return token_res\n",
    "    return apply\n",
    "imdb_label_lookup = {0: \"negative\", 1:\"positive\", -1: \"unknown\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_to_tokens = to_tokens(tokenizer, imdb_label_lookup)\n",
    "tokenized_dataset = imdb_ds.map(imdb_to_tokens, batched=True, removed_columns=[\"text\",\"label\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
