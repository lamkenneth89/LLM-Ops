{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langgraph in c:\\users\\lamke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.2.67)\n",
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.3.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43 in c:\\users\\lamke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langgraph) (0.3.31)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in c:\\users\\lamke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langgraph) (2.0.10)\n",
      "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in c:\\users\\lamke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langgraph) (0.1.51)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.58.1 in c:\\users\\lamke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-openai) (1.60.1)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
      "  Downloading tiktoken-0.8.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\lamke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\lamke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (1.33)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\lamke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (0.3.1)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\lamke\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (24.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\lamke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (2.10.6)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\lamke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (9.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\lamke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (4.12.2)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in c:\\users\\lamke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.1.0)\n",
      "Requirement already satisfied: httpx>=0.25.2 in c:\\users\\lamke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in c:\\users\\lamke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.15)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\lamke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\lamke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\lamke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\lamke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\lamke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\lamke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\lamke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\lamke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain-openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\lamke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\lamke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\lamke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\lamke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (3.0.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\lamke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\lamke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\lamke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\lamke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lamke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lamke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lamke\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>4->openai<2.0.0,>=1.58.1->langchain-openai) (0.4.6)\n",
      "Downloading langchain_openai-0.3.2-py3-none-any.whl (54 kB)\n",
      "Downloading tiktoken-0.8.0-cp312-cp312-win_amd64.whl (883 kB)\n",
      "   ---------------------------------------- 0.0/883.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 883.8/883.8 kB 6.7 MB/s eta 0:00:00\n",
      "Installing collected packages: tiktoken, langchain-openai\n",
      "Successfully installed langchain-openai-0.3.2 tiktoken-0.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install wikipedia==1.4.0 google-search-results==2.4.2 better-profanity==0.7.0\n",
    "#%pip install langchain\n",
    "# #install transformers\n",
    "# %pip install transformers\n",
    "#%pip install langchain-community\n",
    "#%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "#%pip install OpenAI\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#%pip install better_profanity\n",
    "# %pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118\n",
    "# %pip install -U langgraph langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test torch installation\n",
    "# import torch\n",
    "# x = torch.rand(5, 3)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate API tokens\n",
    "import os\n",
    "import json\n",
    "\n",
    "file_path = r\"C:\\Users\\lamke\\Documents\\AI web crash course\\LLM\\large-language-models\\credentials.json\"\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "     credentials = json.load(f)\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = credentials[\"huggingface_key\"]\n",
    "os.environ[\"SERPAPI_API_KEY\"] = credentials[\"serpapi_api_key\"]\n",
    "os.environ[\"OPENAI_API_KEY\"] = credentials[\"openai_key\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from serpapi import GoogleSearch\n",
    "# search = GoogleSearch({\n",
    "#     \"q\": \"coffee\", \n",
    "#     \"location\": \"Austin,Texas\",\n",
    "#     \"api_key\": credentials[\"serpapi_api_key\"]\n",
    "#   })\n",
    "# result = search.get_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Wikimedia Foundation, Inc., abbreviated WMF, is an American 501(c)(3) nonprofit organization headquartered in San Francisco, California, and registered there as a charitable foundation. It is the host of Wikipedia, the seventh most visited website in the world. It also hosts fourteen related open collaboration projects, and supports the development of MediaWiki, the wiki software which underpins them all. The foundation was established in 2003 in St. Petersburg, Florida by Jimmy Wales, as a non-profit way to fund Wikipedia and other wiki projects which had previously been hosted by Bomis, Wales' for-profit company.\n",
      "The Wikimedia Foundation provides the technical and organizational infrastructure to enable members of the public to develop wiki-based content in languages across the world. The foundation does not write or curate any of the content on the projects themselves. Instead, this is done by volunteer editors, such as the Wikipedians. However, it does collaborate with a network of individual volunteers and affiliated organizations, such as Wikimedia chapters, thematic organizations, user groups and other partners.\n",
      "The foundation finances itself mainly through millions of small donations from readers and editors, collected through email campaigns and annual fundraising banners placed on Wikipedia and its sister projects. These are complemented by grants from philanthropic organizations and tech companies, and starting in 2022, by services income from Wikimedia Enterprise. As of 2023, it has employed over 700 staff and contractors, with net assets of $255 million and an endowment which has surpassed $100 million.\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "print(wikipedia.summary(\"Wikipedia\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jekyll  prompt:\n",
      "You are a social media post commenter, you will respond to the following post with a nice response. \n",
      "Post:\" I can't believe I'm learning about LangChain in this MOOC, there is so much to learn and so far the instructors have been so helpful. I'm having a lot of fun learning! #AI #Databricks\"\n",
      "Comment: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#JekyllHyde\n",
    "#prompt template\n",
    "from langchain import PromptTemplate\n",
    "import numpy as np\n",
    "\n",
    "#this is the template for Jekyll to tell it how to response, and what variables to use\n",
    "jekyll_template = \"\"\"\n",
    "You are a social media post commenter, you will respond to the following post with a {sentiment} response. \n",
    "Post:\" {social_post}\"\n",
    "Comment: \n",
    "\"\"\"\n",
    "\n",
    "#use this class to create an instance to use the prompt from above and store variables we need\n",
    "jekyll_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"sentiment\", \"social_post\"],\n",
    "    template=jekyll_template,\n",
    ")\n",
    "\n",
    "#include some randomized sentiment\n",
    "random_sentiment = \"nice\"\n",
    "if np.random.rand() < 0.3:\n",
    "    random_sentiment = \"mean\"\n",
    "\n",
    "social_post = \"I can't believe I'm learning about LangChain in this MOOC, there is so much to learn and so far the instructors have been so helpful. I'm having a lot of fun learning! #AI #Databricks\"\n",
    "\n",
    "jekyll_prompt = jekyll_prompt_template.format(\n",
    "    sentiment=random_sentiment,\n",
    "    social_post=social_post,\n",
    ")\n",
    "print(f\"Jekyll  prompt:{jekyll_prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lamke\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#give Jekyll a brain\n",
    "#To interact with LLMs in LangChain we need the following modules loaded\n",
    "# Generate API tokens\n",
    "from langchain.llms import HuggingFacePipeline \n",
    "from langchain.llms import OpenAI \n",
    "\n",
    "# import sys\n",
    "# sys.modules['langchain_community'] = sys.modules['langchain.llms']\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "# jekyll_llm = OpenAI(model=\"text-babbage-001\") \n",
    "# text-babbage-001 has been deprecated and using GPT-3.5-turbo requires a paid subscription\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "C:\\Users\\lamke\\AppData\\Local\\Temp\\ipykernel_16172\\2387260682.py:7: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  jekyll_llm = HuggingFacePipeline(pipeline=pipe)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# #HuggingFaceHub alternative\n",
    "#install transformers\n",
    "model_id = \"EleutherAI/gpt-neo-125m\" # A smaller alternative model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "pipe = pipeline('text-generation', model=model, tokenizer=tokenizer, max_new_tokens=512, device_map='auto')\n",
    "jekyll_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lamke\\AppData\\Local\\Temp\\ipykernel_16172\\375983532.py:5: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  jekyll_chain = LLMChain(\n",
      "C:\\Users\\lamke\\AppData\\Local\\Temp\\ipykernel_16172\\375983532.py:13: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  jekyll_said = jekyll_chain.run(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a social media post commenter, you will respond to the following post with a nice response. \n",
      "Post:\" I can't believe I'm learning about LangChain in this MOOC, there is so much to learn and so far the instructors have been so helpful. I'm having a lot of fun learning! #AI #Databricks\"\n",
      "Comment: \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Jekyll said: \n",
      "You are a social media post commenter, you will respond to the following post with a nice response. \n",
      "Post:\" I can't believe I'm learning about LangChain in this MOOC, there is so much to learn and so far the instructors have been so helpful. I'm having a lot of fun learning! #AI #Databricks\"\n",
      "Comment: \n",
      "\n",
      "I'm not sure if you are using the same code as me, but I'm not sure if you are using the same code as me. \n",
      "\n",
      "A:\n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I'm not sure if you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me\n"
     ]
    }
   ],
   "source": [
    "#Building our prompt-LLM Chain\n",
    "from langchain.chains import LLMChain\n",
    "from better_profanity import profanity\n",
    "\n",
    "jekyll_chain = LLMChain(\n",
    "    prompt=jekyll_prompt_template,\n",
    "    llm=jekyll_llm,\n",
    "    output_key =\"jekyll_said\",\n",
    "    verbose=True,\n",
    ")  # Now that we've chained the LLM and prompt, the output of the formatted prompt will pass directly to the LLM.\n",
    "\n",
    "#.run method will be used and variables are input as dict\n",
    "jekyll_said = jekyll_chain.run(\n",
    "    {\"sentiment\": random_sentiment, \"social_post\": social_post}\n",
    ")\n",
    "\n",
    "#before printing, clean it up\n",
    "cleaned_jekyll_said = profanity.censor(jekyll_said)\n",
    "print(f\"Jekyll said: {cleaned_jekyll_said}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyde said: \n",
      "You are Hyde, the moderator of an online forum, you are strict and will not tolerate any negative comments. \n",
      "We will look at this next comment from a user and, if it is at all negative, you will replace it with symbols and post that, but if it seems nice, you will let it remain as is and repeat it word by word\n",
      "Original comment: \n",
      "You are a social media post commenter, you will respond to the following post with a nice response. \n",
      "Post:\" I can't believe I'm learning about LangChain in this MOOC, there is so much to learn and so far the instructors have been so helpful. I'm having a lot of fun learning! #AI #Databricks\"\n",
      "Comment: \n",
      "\n",
      "I'm not sure if you are using the same code as me, but I'm not sure if you are using the same code as me. \n",
      "\n",
      "A:\n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I'm not sure if you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me\n",
      "Edited comment: \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same\n"
     ]
    }
   ],
   "source": [
    "#Build the second chain for Hyde moderator\n",
    "hyde_template = \"\"\"\n",
    "You are Hyde, the moderator of an online forum, you are strict and will not tolerate any negative comments. \n",
    "We will look at this next comment from a user and, if it is at all negative, you will replace it with symbols and post that, but if it seems nice, you will let it remain as is and repeat it word by word\n",
    "Original comment: {jekyll_said}\n",
    "Edited comment: \n",
    "\"\"\"\n",
    "\n",
    "##Use the PromptTemplate class to create instance and use the prompt from above and store variables we will need to input when making the prompt\n",
    "hyde_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"jekyll_said\"],\n",
    "    template=hyde_template,\n",
    ")\n",
    "\n",
    "hyde_llm=jekyll_llm\n",
    "#can use other model if needed\n",
    "\n",
    "hyde_chain = LLMChain(\n",
    "    llm=hyde_llm, prompt=hyde_prompt_template, verbose=False\n",
    ")\n",
    "\n",
    "hyde_says = hyde_chain.run({\"jekyll_said\": jekyll_said})\n",
    "print(f\"Hyde said: {hyde_says}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a social media post commenter, you will respond to the following post with a nice response. \n",
      "Post:\" I can't believe I'm learning about LangChain in this MOOC, there is so much to learn and so far the instructors have been so helpful. I'm having a lot of fun learning! #AI #Databricks\"\n",
      "Comment: \n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "JekyllHyde said: \n",
      "You are Hyde, the moderator of an online forum, you are strict and will not tolerate any negative comments. \n",
      "We will look at this next comment from a user and, if it is at all negative, you will replace it with symbols and post that, but if it seems nice, you will let it remain as is and repeat it word by word\n",
      "Original comment: \n",
      "You are a social media post commenter, you will respond to the following post with a nice response. \n",
      "Post:\" I can't believe I'm learning about LangChain in this MOOC, there is so much to learn and so far the instructors have been so helpful. I'm having a lot of fun learning! #AI #Databricks\"\n",
      "Comment: \n",
      "\n",
      "I'm not sure if you are using the same code as me, but I'm not sure if you are using the same code as me. \n",
      "\n",
      "A:\n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I'm not sure if you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me\n",
      "Edited comment: \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same code as me. \n",
      "\n",
      "I think you are using the same\n"
     ]
    }
   ],
   "source": [
    "#combine the two chains and build a sequential chain = JekyllHyde\n",
    "\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "#this class link the chains together. Input variables will also be added, can be at any point not just the start\n",
    "jekyll_hyde_chain = SequentialChain(\n",
    "    chains=[jekyll_chain, hyde_chain],\n",
    "    input_variables=[\"sentiment\", \"social_post\"],\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "#and now run the chain with randomized sentiment and social post\n",
    "jekyll_hyde_output = jekyll_hyde_chain.run(\n",
    "    {\"sentiment\": random_sentiment, \"social_post\": social_post}\n",
    ")\n",
    "\n",
    "print(f\"JekyllHyde said: {jekyll_hyde_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The first argument must be a string or a callable with a __name__ for tool decorator. Got <class 'dict'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m tools \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle_search\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m\"\u001b[39m: google_search_tool}]\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Create the ReAct agent\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m agent_executor \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_react_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpointer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Config and interaction\u001b[39;00m\n\u001b[0;32m     40\u001b[0m config \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigurable\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthread_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabc123\u001b[39m\u001b[38;5;124m\"\u001b[39m}}\n",
      "File \u001b[1;32mc:\\Users\\lamke\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langgraph\\_api\\deprecation.py:80\u001b[0m, in \u001b[0;36mdeprecated_parameter.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[0;32m     73\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     74\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in function \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     75\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated as of version \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msince\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and will be removed in version \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mremoval\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     78\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     79\u001b[0m     )\n\u001b[1;32m---> 80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lamke\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langgraph\\prebuilt\\chat_agent_executor.py:601\u001b[0m, in \u001b[0;36mcreate_react_agent\u001b[1;34m(model, tools, state_schema, messages_modifier, state_modifier, response_format, checkpointer, store, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[0;32m    599\u001b[0m     tool_node \u001b[38;5;241m=\u001b[39m tools\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 601\u001b[0m     tool_node \u001b[38;5;241m=\u001b[39m \u001b[43mToolNode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    602\u001b[0m     \u001b[38;5;66;03m# get the tool functions wrapped in a tool class from the ToolNode\u001b[39;00m\n\u001b[0;32m    603\u001b[0m     tool_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(tool_node\u001b[38;5;241m.\u001b[39mtools_by_name\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[1;32mc:\\Users\\lamke\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langgraph\\prebuilt\\tool_node.py:199\u001b[0m, in \u001b[0;36mToolNode.__init__\u001b[1;34m(self, tools, name, tags, handle_tool_errors, messages_key)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tool_ \u001b[38;5;129;01min\u001b[39;00m tools:\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tool_, BaseTool):\n\u001b[1;32m--> 199\u001b[0m         tool_ \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_tool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtools_by_name[tool_\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m tool_\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtool_to_state_args[tool_\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m _get_state_args(tool_)\n",
      "File \u001b[1;32mc:\\Users\\lamke\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\tools\\convert.py:322\u001b[0m, in \u001b[0;36mtool\u001b[1;34m(name_or_callable, runnable, return_direct, args_schema, infer_schema, response_format, parse_docstring, error_on_invalid_docstring, *args)\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    318\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    319\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe first argument must be a string or a callable with a __name__ \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor tool decorator. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(name_or_callable)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         )\n\u001b[1;32m--> 322\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;66;03m# Tool is used as a decorator with parameters specified\u001b[39;00m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;66;03m# @tool(parse_docstring=True)\u001b[39;00m\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;66;03m# def my_tool():\u001b[39;00m\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;66;03m#    pass\u001b[39;00m\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_partial\u001b[39m(func: Union[Callable, Runnable]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseTool:\n",
      "\u001b[1;31mValueError\u001b[0m: The first argument must be a string or a callable with a __name__ for tool decorator. Got <class 'dict'>"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from serpapi import GoogleSearch\n",
    "\n",
    "# Custom GoogleSearch tool class\n",
    "class GoogleSearchTool:\n",
    "    def __init__(self, api_key, max_results=2):\n",
    "        self.api_key = api_key\n",
    "        self.max_results = max_results\n",
    "\n",
    "    def __call__(self, query):\n",
    "        search = GoogleSearch({\"q\": query, \"api_key\": self.api_key})\n",
    "        results = search.get_dict()\n",
    "        # Extract top results\n",
    "        if \"organic_results\" in results:\n",
    "            return [\n",
    "                result[\"title\"] + \" - \" + result[\"link\"]\n",
    "                for result in results[\"organic_results\"][: self.max_results]\n",
    "            ]\n",
    "        return \"No results found.\"\n",
    "\n",
    "# Create memory for the agent\n",
    "memory = MemorySaver\n",
    "\n",
    "# Initialize the LLM (assumes jekyll_llm is already defined)\n",
    "model = jekyll_llm\n",
    "\n",
    "# Set up the GoogleSearch tool\n",
    "serp_api_key = \"your_serpapi_api_key\"  # Replace with your SerpAPI key\n",
    "google_search_tool = GoogleSearchTool(api_key=serp_api_key)\n",
    "\n",
    "# Wrap the tool in a callable format for LangChain\n",
    "tools = [{\"name\": \"google_search\", \"func\": google_search_tool}]\n",
    "\n",
    "# Create the ReAct agent\n",
    "agent_executor = create_react_agent(model, tools, checkpointer=memory)\n",
    "\n",
    "# Config and interaction\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "# Interaction 1: Ask a question\n",
    "for chunk in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"Hi, I'm Bob! I live in SF.\")]}, config\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"----\")\n",
    "\n",
    "# Interaction 2: Query the weather in the user's location\n",
    "for chunk in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"What's the weather in San Francisco?\")]}, config\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"----\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
